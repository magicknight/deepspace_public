[summary]
    # name of this experiment
    name = 'tpu_gan2d_aug_npy'
    author = 'Zhihua Liang'
    discription = 'Bone reconstruction on TPU'
    # root directory of the project (for saveing logs, models, temp files, etc...)
    project_root = '/home/zhihua/work'    
[deepspace]
    ########### agent ###########
    # agent name
    agent = 'deepspace.agents.bone.tpu_gan2d_aug.Agent'
    # which device to use, can be 'gpu', 'tpu', or 'cpu'
    device = 'cpu'
    # distribute setting
    world_size = 1
    # random seed
    seed = 999
    # Learning rate, can be a float number like 0.001 or a list, like [0.1, 0.001, 1e-6]
    gen_lr = 1e-2
    dis_lr = 1e-2
    # data augment
    policy = ['color', 'translation', 'cutout']
    # scheduler
    T_0 = 8
    T_mult = 2
    lr_decay = 0.999
    
    # image loss and dis loss weight
    loss_weight = 0.999999
    # max epoches to for training.
    max_epoch = 10001
    # mode = 'train', 'test', 'validate'
    mode = 'test'

    ########### dataset ###########
    # dataset name
    # train_dataset = '/home/zhihua/gcs/deepspace/bone/npy/aligned_LQ.npy'
    # target_dataset = '/home/zhihua/gcs/deepspace/bone/npy/aligned_HQ.npy'
    train_dataset = '/home/zhihua/gcs/deepspace/bone/npy/aligned_LQ.npy'
    target_dataset = '/home/zhihua/gcs/deepspace/bone/npy/aligned_HQ.npy'    
    padding = false
    # load into memory before distributing
    shared_dataset = false    
    # batch
    train_batch = 4
    validate_batch = 4
    # validate length
    validation_size = 256
    # workers for dataloader
    data_loader_workers = 8
    # image size and channel
    image_channels = 1
    image_size = [768, 768]
    # data argument
    drop_probability = [0.05, 0.3]
    break_probability = 0.5
    min_erasing_area = 0.01
    max_erasing_area = 0.1
    min_aspect_ratio = 0.3
    value = 0
    rotation_node = 4
    gaussian_mean = 0
    gaussian_std = 0.01
    # dataloader options
    drop_last = true
    shuffle = true
    # parallal loader options
    batchdim = 0
    loader_prefetch_size = 16
    device_prefetch_size = 8

    ########## model ############
    # validate every x epoches
    validate_step = 10    
    # save model every x steps
    save_model_step = 10
    backup_model_step = 30    
    # save ouput on first n images during validation
    save_images = [0, 3]
    save_image_step = 10
    # checkpoint file name
    checkpoint_file = 'checkpoint.pth.tar'
    # generator model
    gen_encoder_sizes = [8, 12, 32, 48, 64, 96, 128, 192]
    gen_temporal_strides = [2, 2, 2, 2, 2, 2, 2, 2]
    gen_fc_sizes = [1536]
    # discriminator model
    dis_encoder_sizes = [8, 12, 32, 48, 64, 96, 128, 192]
    dis_temporal_strides = [2, 2, 2, 2, 2, 2, 2, 2]
    dis_fc_sizes = [16, 1]
    # output format; png or tif
    data_format = 'npy'

    

    ##### test ######
    test_dataset = '/home/zhihua/gcs/deepspace/bone/npy/aligned_LQ.npy'
    test_batch = 128
    test_output_dir = '/home/zhihua/work/tpu_gan2d_aug_npy/out'
    # select part of the data
    image_range = [100, 228]


[common]
    distribute = false
    # log level, can be INFO, WARNING, ERROR..., see python logging
    log_level = 'INFO'




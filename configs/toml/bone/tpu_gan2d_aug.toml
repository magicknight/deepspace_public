[summary]
    # name of this experiment
    name = 'tpu_gan2d_aug_npy'
    author = 'Zhihua Liang'
    discription = 'Bone reconstruction on TPU'
    # root directory of the project (for saveing logs, models, temp files, etc...)
    project_root = '/home/zhihua/data/work_space/bone'    
[deepspace]
    ########### agent ###########
    # agent name
    agent = 'deepspace.agents.bone.tpu_gan2d_aug.Agent'
    # which device to use, can be 'gpu', 'tpu', or 'cpu'
    device = 'tpu'
    # distribute setting
    world_size = 8
    # random seed
    seed = 999
    # Learning rate, can be a float number like 0.001 or a list, like [0.1, 0.001, 1e-6]
    gen_lr = 1e-1
    dis_lr = 1e-1
    # data augment
    policy = ['color', 'offset', 'translation', 'cutout']
    # scheduler
    T_0 = 10
    T_mult = 3
    lr_decay = 0.999
    scheduler = 'WarmupAndExponentialDecayScheduler'
    scheduler_divisor = 5
    scheduler_divide_every_n_epochs = 20  
    # image loss and dis loss weight
    loss_weight = 0.999
    # max epoches to for training.
    max_epoch = 1000
    # mode = 'train', 'test', 'validate'
    mode = 'train'

    ########### dataset ###########
    # dataset name
    train_dataset = [
        '/home/zhihua/ssd/npy/aligned_LQ.npy',
        '/home/zhihua/ssd/npy/aligned_HQ.npy',
    ]
    padding = false
        # '/home/zhihua/storage/impainting/bone_reconstruction/yves_2020_10_21/npy/recon_noisy.npy', 
        # '/home/zhihua/storage/impainting/bone_reconstruction/yves_2020_10_21/npy/recon_original.npy',
        # '/home/zhihua/storage/impainting/bone_reconstruction/yves_2020_10_21/npy/validate_noisy.npy',
        # '/home/zhihua/storage/impainting/bone_reconstruction/yves_2020_10_21/npy/validate_origin.npy'
    
    # load into memory before distributing
    shared_dataset = true    
    # batch
    train_batch = 2
    validate_batch = 2
    # validate length
    validation_size = 64
    # workers for dataloader
    data_loader_workers = 16
    # image size and channel
    image_channels = 1
    image_size = [768, 768]
    # data argument
    drop_probability = [0.05, 0.3]
    break_probability = 0.5
    min_erasing_area = 0.01
    max_erasing_area = 0.2
    min_aspect_ratio = 0.3
    value = 0
    # dataloader options
    drop_last = true
    shuffle = false


    ########## model ############
    # save model every x steps
    save_model_step = 5
    save_checkpoint_step = 10
    save_image_step = 1
    # checkpoint file name
    checkpoint_file = 'checkpoint.pth.tar'
    # generator model
    gen_encoder_sizes = [2, 4, 8, 16, 32, 64, 96, 128]
    gen_temporal_strides = [2, 2, 2, 2, 2, 2, 2, 2]
    gen_fc_sizes = [1024]
    # discriminator model
    dis_encoder_sizes = [2, 4, 8, 16, 32, 64, 96, 128]
    dis_temporal_strides = [2, 2, 2, 2, 2, 2, 2, 2]
    dis_fc_sizes = [64, 16, 1]
    # output format; png or tif
    data_format = 'npy'
    # save ouput on first n images during validation
    save_images = [0, 6]
    

    ##### test ######
    test_dataset = '/home/zhihua/ssd/npy/aligned_LQ.npy'
    test_batch = 16
    test_output_dir = '/home/zhihua/data/work_space/bone/aligned_gan3d_test'

[common]
    distribute = true
    # log level, can be INFO, WARNING, ERROR..., see python logging
    log_level = 'WARNING'



